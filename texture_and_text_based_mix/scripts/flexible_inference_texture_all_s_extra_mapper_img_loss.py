import os
import sys
sys.path.append(".")
sys.path.append("..")

import copy
import clip
import numpy as np

import torch
import torchvision
from models.stylegan2.model import Generator
from delta_mapper_all_s_extra_mapper import DeltaMapper
from utils import stylespace_util_all_s

import argparse
import re
from typing import List
from tqdm import tqdm
from PIL import Image
from utils import map_tool

def num_range(s: str) -> List[int]:
    '''Accept either a comma separated list of numbers 'a,b,c' or a range 'a-c' and return as a list of ints.'''

    range_re = re.compile(r'^(\d+)-(\d+)$')
    m = range_re.match(s)
    if m:
        return list(range(int(m.group(1)), int(m.group(2))+1))
    vals = s.split(',')
    return [int(x) for x in vals]

def listdir(path, list_name):
    for file in os.listdir(path):
        file_path = os.path.join(path, file)
        list_name.append(file_path)

#å°†ç›®æ ‡çº¹ç†å›¾è½¬æˆCLIPç‰¹å¾ç©ºé—´ä¸­çš„ç‰¹å¾å‘é‡ï¼Œä¸åŸå§‹çº¹ç†çš„ç‰¹å¾å‘é‡ç›¸å‡ï¼Œæ±‚Delta c
def getClipFromImage(image_path,ori_texture_c,clip_model,preprocess,device,index,save_path,save_name):
    image = Image.open(image_path).convert('RGB')
    image.save(os.path.join(save_path,save_name, str(index)+"_"+"texture.jpg"))
    image = preprocess(image).unsqueeze(0).to(device)
    target_texture_c = clip_model.encode_image(image)
    target_texture_c = target_texture_c / target_texture_c.norm(dim=-1, keepdim=True).float()
    delta_c = target_texture_c[0] - ori_texture_c[0]
    delta_c = delta_c / delta_c.norm(dim=-1, keepdim=True).float().clamp(min=1e-5)
    delta_c = torch.cat([ori_texture_c[0], delta_c], dim=0)
    return delta_c.unsqueeze(0)
    

def main(opts):

    device = "cuda" if torch.cuda.is_available() else "cpu"

    #Initialize generator
    print('Loading stylegan weights from pretrained!')
    g_ema = Generator(size=256, style_dim=512, n_mlp=2)
    g_ema_ckpt = torch.load(opts.stylegan_weights)
    g_ema.load_state_dict(g_ema_ckpt['g_ema'], strict=False)
    g_ema.eval()
    g_ema = g_ema.to(device)

    #Initialze DeltaMapper
    net = DeltaMapper()
    net_ckpt = torch.load(opts.checkpoint_path)
    net.load_state_dict(net_ckpt)
    net = net.to(device)
    
    #Load CLIP model
    clip_model, preprocess = clip.load("ViT-B/32", device=device)

    os.makedirs(os.path.join(opts.save_path,opts.save_name), exist_ok=True)

    #Initialize test dataset
    batch_size_latent=torch.zeros([1])
    s_latents_list=[]
    delta_c_list=[]
    dt_list = []
    ori_texture_c_list=[]
    # é‡‡æ ·è¡£æœ
    print("sampling cloths")
    for num in tqdm(range(len(opts.cloth_ids)),position=0):
        w_path=os.path.join(opts.sample_w_path,str(opts.cloth_ids[num])+".npy")
        latent_code=np.load(w_path, allow_pickle=True)
        w = torch.from_numpy(latent_code).to(device).unsqueeze(0)
        #ä½¿ç”¨wç”Ÿæˆsï¼ˆåŒ…å«toRGBå±‚ï¼‰
        style_space, noise = stylespace_util_all_s.encoder_latent(g_ema, w)
        s = torch.cat(style_space, dim=1)
        s_latents_list.append(s)
        # è·å–é‡‡æ ·æœè£…çš„çº¹ç†å›¾(å³åŸå§‹çº¹ç†)å¹¶æ˜ å°„åˆ°CLIPç‰¹å¾ç©ºé—´ä¸­
        ori_texture=Image.open(os.path.join(opts.sample_cloth_texture_path,str(opts.cloth_ids[num])+".jpg"))
        ori_texture = preprocess(ori_texture).unsqueeze(0).to(device)
        ori_texture_c = clip_model.encode_image(ori_texture)
        ori_texture_c = ori_texture_c / ori_texture_c.norm(dim=-1, keepdim=True).float()
        ori_texture_c_list.append(ori_texture_c)
    # æå–ç›®æ ‡çº¹ç†å›¾çš„ç‰¹å¾
    if opts.texture_ids[0]!=-1:#æ ¹æ®ç»™å®šçš„çº¹ç†å›¾idåºåˆ—è¯»å–çº¹ç†å›¾
        print("Loading specified texture") 
        for num in tqdm(range(len(opts.texture_ids)),position=0):
            for cloth_num in range(len(ori_texture_c_list)):
                # image_path=os.path.join(opts.texture_path,str(num).zfill(2) +".png")
                # image_path=os.path.join(opts.texture_path,str(opts.texture_ids[num]) +".png")
                # image_path=os.path.join(opts.texture_path,str(num).zfill(3) +".jpg")
                image_path=os.path.join(opts.texture_path,str(opts.texture_ids[num]) +".jpg") # image_pathæ˜¯ç›®æ ‡çº¹ç†å›¾çš„è·¯å¾„
                delta_c = getClipFromImage(image_path,ori_texture_c_list[cloth_num],clip_model,preprocess,device,num,opts.save_path,opts.save_name)
                delta_c_list.append(delta_c)
    else:#è¯»å–æ–‡ä»¶å¤¹å†…çš„æ‰€æœ‰çº¹ç†å›¾
        print("Loading all textures in folder")
        path_list = []
        listdir(opts.texture_path, path_list)
        count=-1
        for image_path in tqdm(path_list):#texture
            count+=1
            # ä¸‹é¢è¿™å‡ è¡Œæ˜¯æ‰‹åŠ¨åˆ†æ‰¹æ¬¡è¯»å–çº¹ç†å›¾ï¼Œå› ä¸ºä¸€ä¸‹å­è¯»å®Œæ‰€æœ‰ä¼šçˆ†æ˜¾å­˜
            if count>=0 and count<100:
            # if count>=100 and count<200:
            # if count>=200 and count<300:
            # if count>=300 and count<400:
            # if count>=400 and count<470:
                for cloth_num in range(len(ori_texture_c_list)):#sample cloth
                    delta_c = getClipFromImage(image_path,ori_texture_c_list[cloth_num],clip_model,preprocess,device,count,opts.save_path,opts.save_name)
                    delta_c_list.append(delta_c)
    print("len(delta_c_list):",len(delta_c_list))
    #å¦‚æœè¾“å…¥äº†æ–‡æœ¬ï¼Œåˆ™æå–æ–‡æœ¬çº¹ç†ç‰¹å¾
    if opts.target_text is not None:
        neutral=opts.neutral
        target_list = opts.target_text.split(',')
        # target_name=opts.target_text.replace(' ', '-').replace(',', '_')
        for target in target_list:
            classnames=[target,neutral]
            dt = map_tool.GetDt(classnames,clip_model)
            dt = torch.Tensor(dt).to(device)
            dt = dt / dt.norm(dim=-1, keepdim=True).float().clamp(min=1e-5)
            dt_list.append(dt)

    print("generating result")
    for texture_index, delta_c in tqdm(enumerate(delta_c_list)): #t0s0,t0s1,t0s2,t1s0,t1s1,t1s2
        cloth_index=texture_index%len(s_latents_list)
        latent_s=s_latents_list[cloth_index]
        with torch.no_grad():
            fake_delta_s = net(latent_s, delta_c)
            improved_fake_delta_s = copy.copy(fake_delta_s[0])
            #delta_sæ˜¯æ ¹æ®çº¹ç†å›¾å¾—åˆ°çš„åç§»å‘é‡
            delta_s = improved_fake_delta_s.unsqueeze(0) 
            if opts.target_text is not None:#å¦‚æœç”¨æ–‡æœ¬æ§åˆ¶é¢œè‰²åˆ™è¿›å…¥æ­¤åˆ†æ”¯
                for text_index, dt in enumerate(dt_list):
                    delta_c_text=torch.zeros_like(delta_c)
                    delta_c_text[0, :512]=delta_c[0, :512]
                    delta_c_text[0, 512:] = dt
                    #fake_delta_s_textæ˜¯æ ¹æ®æ–‡æœ¬å¾—åˆ°çš„åç§»å‘é‡
                    fake_delta_s_text = net(latent_s, delta_c_text)
                    #ç”±äºæ–‡æœ¬æ˜¯ç”¨äºæ§åˆ¶é¢œè‰²çš„ï¼Œæ‰€ä»¥ä»¤fake_delta_s_texté™¤äº†è¦è¾“å…¥åˆ°TORGBå±‚ä»¥å¤–çš„éƒ¨åˆ†éƒ½ä¸º0
                    #coarse
                    fake_delta_s_text[:, :4*512]=0
                    #medium
                    fake_delta_s_text[:, 5*512:7*512]=0
                    fake_delta_s_text[:, 8*512:10*512]=0
                    #fine
                    fake_delta_s_text[:, 11*512:12*512+256]=0
                    fake_delta_s_text[:, 12*512+256*2 : 12*512 + 256*3 + 128]=0
                    fake_delta_s_text[:, 12*512 + 256*3 + 128*2 : 12*512 + 256*3+ 128*3 + 64]=0
                    #ç”±äºçº¹ç†å›¾æ˜¯ç”¨äºæ§åˆ¶èŠ±çº¹çš„ï¼Œæ‰€ä»¥ä»¤delta_sè¦è¾“å…¥åˆ°TORGBå±‚çš„éƒ¨åˆ†éƒ½ä¸º0
                    #RGB
                    delta_s[:, 4*512:5*512]=0
                    delta_s[:, 7*512:8*512]=0
                    delta_s[:, 10*512:11*512]=0
                    delta_s[:, 12*512+256:12*512+256*2]=0
                    delta_s[:, 12*512 + 256*3 + 128:12*512 + 256*3 + 128*2]=0
                    delta_s[:, 12*512 + 256*3 + 128*3 + 64:]=0
                    #ç”Ÿæˆç»“æœå¹¶ä¿å­˜å›¾åƒ
                    img_gen = stylespace_util_all_s.decoder_validate(g_ema, latent_s + delta_s + fake_delta_s_text, batch_size_latent)
                    torchvision.utils.save_image(img_gen, os.path.join(opts.save_path,opts.save_name, str(texture_index//len(opts.cloth_ids))+"_"+str(cloth_index)+"_"+target_list[text_index]+".jpg"), normalize=True, range=(-1, 1))
                    #ä¿å­˜æœ€ç»ˆçš„ç‰¹å¾å‘é‡
                    s_cpu_0=[]
                    s_out_0=stylespace_util_all_s.split_stylespace_256(latent_s)
                    for i in range(20):
                        s_cpu_0.append(s_out_0[i].cpu())
                    np.save(os.path.join(opts.save_path,opts.save_name, str(cloth_index)+"_orig.npy"),s_cpu_0)
                    s_cpu=[]
                    s_out=latent_s + delta_s + fake_delta_s_text
                    s_out=stylespace_util_all_s.split_stylespace_256(s_out)
                    for i in range(20):
                        s_cpu.append(s_out[i].cpu())
                    #è‹¥æ‰‹åŠ¨åˆ†æ‰¹æ¬¡å¤„ä¸æ˜¯if count>=0 and count<100:ï¼Œåˆ™éœ€è¦å†ä¸‹é¢çš„texture_indexå‰åŠ æ•°ï¼Œæ¯”å¦‚åœ¨ä½¿ç”¨if count>=100 and count<200æ—¶ï¼Œä¿å­˜çš„æ–‡ä»¶ååº”ä¸ºos.path.join(opts.save_path,opts.save_name, str(100+texture_index//len(opts.cloth_ids))+"_"+str(cloth_index)+"_"+target_list[text_index]+".npy")
                    np.save(os.path.join(opts.save_path,opts.save_name, str(texture_index//len(opts.cloth_ids))+"_"+str(cloth_index)+"_"+target_list[text_index]+".npy"),s_cpu)
            else:#å¦‚æœä¸ä½¿ç”¨æ–‡æœ¬åˆ™è¿›å…¥æ­¤åˆ†æ”¯
                #ç”Ÿæˆç»“æœå¹¶ä¿å­˜å›¾åƒ
                img_gen = stylespace_util_all_s.decoder_validate(g_ema, latent_s + delta_s, batch_size_latent)
                torchvision.utils.save_image(img_gen, os.path.join(opts.save_path,opts.save_name, str(texture_index//len(opts.cloth_ids))+"_"+str(cloth_index)+".jpg"), normalize=True, range=(-1, 1))
                #ä¿å­˜æœ€ç»ˆçš„ç‰¹å¾å‘é‡
                s_cpu=[]
                s_out=latent_s + delta_s
                s_out=stylespace_util_all_s.split_stylespace_256(s_out)
                for i in range(20):
                    s_cpu.append(s_out[i].cpu())
                #è‹¥æ‰‹åŠ¨åˆ†æ‰¹æ¬¡å¤„ä¸æ˜¯if count>=0 and count<100:ï¼Œåˆ™éœ€è¦å†ä¸‹é¢çš„texture_indexå‰åŠ æ•°ï¼Œæ¯”å¦‚åœ¨ä½¿ç”¨if count>=100 and count<200æ—¶ï¼Œä¿å­˜çš„æ–‡ä»¶ååº”ä¸ºos.path.join(opts.save_path,opts.save_name, str(100+texture_index//len(opts.cloth_ids))+"_"+str(cloth_index)+".npy"))
                np.save(os.path.join(opts.save_path,opts.save_name, str(texture_index//len(opts.cloth_ids))+"_"+str(cloth_index)+".npy"),s_cpu)
    print(f'completedğŸ‘! Please check results in {opts.save_path}/{opts.save_name}')

if __name__ == "__main__":
    parser = argparse.ArgumentParser(description='opt')
    parser.add_argument('-sample_w_path', default='../editGANdata/620t_sample_200000/w', type=str,help='path to sampled w')
    parser.add_argument('-texture_path', default='./', type=str,help='path to texture folder')
    # num_rangeæ˜¯ä¸€ç§è‡ªå®šä¹‰çš„æ•°æ®ç»“æ„ï¼Œå¯ä»¥ç”¨a-cæˆ–a,b,cçš„å½¢å¼è¡¨ç¤ºç›®æ ‡åºå·
    parser.add_argument('-texture_ids', type=num_range, help='ids of texture that needed. If not specified, the entire folder will be traversed', default='-1')
    parser.add_argument('-cloth_ids', type=num_range, help='ids of sampled cloths', default='0')
    parser.add_argument('-target_text', type=str, default= None, help='Specify the target attributes to be edited')
    parser.add_argument('-stylegan_weights', default="../editGAN/cloth-v2-620t.pt", type=str, help='path to stylegan weights')
    parser.add_argument('-checkpoint_path', type=str, default='checkpoints/all_s_extra_mapper_img_loss/resume/620t_sample_200000_all_s/texture_cropped_sample_200000_620t/net_640000.pth', help='path to texture delta mapper')
    parser.add_argument('-save_path', type=str, default='output/flexible')
    parser.add_argument('-save_name', type=str, required=True, help='the name of the folder that save outputs')
    parser.add_argument('-sample_cloth_texture_path', default='../editGANdata/620t_sample_200000/texture_crop', type=str,help='path to the folder that save the texture  images of sampled clothing images')
    parser.add_argument('-neutral', type=str,  help='neutral attribute word')
    
    opt = parser.parse_args()

    main(opt)