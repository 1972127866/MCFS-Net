{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stylegan weights from pretrained!\n",
      "completedğŸ‘! Please check results in output\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python scripts/inference.py \\\n",
    "--stylegan_weights ./models/pretrain/collar_cloth_8385_300t.pt \\\n",
    "--stylegan_size 256\\\n",
    "--checkpoint_path checkpoints/collar_cloth_8385_300t/net_160000.pth \\\n",
    "--classname collar_cloth_8385_300t_test_sample100 \\\n",
    "--sample True\\\n",
    "--num_all 300\\\n",
    "--target \"white top\"\n",
    "# --target \"short top\",\"long top\",\"dress\",\"loose top\",\"tight top\"\n",
    "# --target \"v-neck top\",\"crew neck top\",\"turtleneck top\",\"square neck top\"\n",
    "# --target \"sleeveless top\",\"short sleeve top\",\"half sleeve top\",\"long sleeve top\"\n",
    "# --target \"yellow top\",\"black top\",\"red top\",\"blue top\",\"printed top\",\"striped top\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/pretrain/collar_cloth_8385_300t.pt\n",
      "9it [00:00, 24.38it/s]\n",
      "torch.Size([9, 14, 512])\n",
      "torch.Size([9, 4928])\n",
      "torch.Size([9, 512])\n"
     ]
    }
   ],
   "source": [
    "#å°†ä½¿ç”¨e4eå¾—åˆ°çš„8385çš„9å¼ æµ‹è¯•å›¾çš„ptæ–‡ä»¶è½¬æˆfeat.npyæ–‡ä»¶\n",
    "!CUDA_VISIBLE_DEVICES=0 python get_embedding_codes.py --classname collar_cloth_8385_300t_test_9 \\\n",
    "    --ckpt ./models/pretrain/collar_cloth_8385_300t.pt\\\n",
    "    --size 256\\\n",
    "    --latent_dir ../e4e/collar_cloth_test_9_embed\\\n",
    "    --batch_size 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "./models/pretrain/collar_cloth_8385_300t.pt\n",
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 10/10 [00:01<00:00,  7.30it/s]\n",
      "torch.Size([100, 14, 512])\n",
      "torch.Size([100, 4928])\n",
      "torch.Size([100, 512])\n"
     ]
    }
   ],
   "source": [
    "#é‡‡æ ·ç”Ÿæˆæµ‹è¯•æ•°æ®\n",
    "!CUDA_VISIBLE_DEVICES=0 python generate_codes.py --classname collar_cloth_8385_300t_test_sample100 --samples 100 \\\n",
    "    --ckpt ./models/pretrain/collar_cloth_8385_300t.pt\\\n",
    "    --size 256"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 python scripts/inference_texture.py \\\n",
    "--stylegan_weights ../editGAN/viton_360t.pt \\\n",
    "--stylegan_size 256 \\\n",
    "--checkpoint_path checkpoints/textures_with_edge_restyle/net_010000.pth \\\n",
    "--classname textures_with_edge_restyle_sample_10 \\\n",
    "--edge_classname edges_for_train_restyle_sample_10 \\\n",
    "--img_classname images_sample_10 \\\n",
    "--sample True \\\n",
    "--target \"nothing\" \\\n",
    "--workers 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æµ‹è¯•èƒ½å¦ä»æ— çº¹ç†è½®å»“å›¾ç”Ÿæˆæœ‰çº¹ç†è½®å»“å›¾\n",
    "!CUDA_VISIBLE_DEVICES=0 python scripts/inference_texture.py \\\n",
    "--stylegan_weights ../editGAN/viton_360t.pt \\\n",
    "--stylegan_size 256 \\\n",
    "--checkpoint_path checkpoints/textures_with_edge_restyle/net_010000_09it.pth \\\n",
    "--tex_edge_classname textures_with_edge_restyle_sample_10 \\\n",
    "--edge_classname edges_for_train_restyle_sample_10 \\\n",
    "--img_classname edges_for_train_restyle_sample_10 \\\n",
    "--sample True \\\n",
    "--target \"red\" \\\n",
    "--workers 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stylegan weights from pretrained!\n",
      "completedğŸ‘! Please check results in output\n"
     ]
    }
   ],
   "source": [
    "#æµ‹è¯•èƒ½å¦ä»ç™½è‰²è¡£æœç”Ÿæˆå½©è‰²è¡£æœ\n",
    "!CUDA_VISIBLE_DEVICES=0 python scripts/inference_texture_white.py \\\n",
    "--stylegan_weights ../editGAN/viton_360t.pt \\\n",
    "--stylegan_size 256 \\\n",
    "--checkpoint_path checkpoints/images_sample_200000/texture_cropped_sample_200000/net_150000.pth \\\n",
    "--texture_classname texture_cropped_sample_10 \\\n",
    "--white_classname images_sample_white_10 \\\n",
    "--target \"red\" \\\n",
    "--workers 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stylegan weights from pretrained!\n",
      "completedğŸ‘! Please check results in output\n"
     ]
    }
   ],
   "source": [
    "#æµ‹è¯•ä»ç™½è‰²è¡£æœå˜æˆå½©è‰²è¡£æœ(conditionä¸ºç”¨maskå®ç°çš„50*50çš„çº¹ç†)ï¼ŒlossåŒ…æ‹¬cå’Œsçš„è®­ç»ƒç»“æœ\n",
    "!CUDA_VISIBLE_DEVICES=0 python scripts/inference_texture_white.py \\\n",
    "--stylegan_weights ../editGAN/viton_360t.pt \\\n",
    "--stylegan_size 256 \\\n",
    "--checkpoint_path checkpoints/images_sample_200000/texture_cropBymask_sample_200000/net_720000.pth \\\n",
    "--texture_classname texture_cropBymask_sample_10 \\\n",
    "--white_classname images_sample_white_10 \\\n",
    "--target \"red\" \\\n",
    "--workers 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æµ‹è¯•èƒ½å¦ä»ç™½è‰²è¡£æœç”Ÿæˆå½©è‰²è¡£æœï¼ˆæ¨¡å‹è®­ç»ƒæ•°æ®çš„mixæ˜¯0-7ï¼‰\n",
    "!CUDA_VISIBLE_DEVICES=0 python scripts/inference_texture_white.py \\\n",
    "--stylegan_weights ../editGAN/viton_360t.pt \\\n",
    "--stylegan_size 256 \\\n",
    "--checkpoint_path checkpoints/images_sample_200000/texture_cropped_sample_200000/net_150000.pth \\\n",
    "--texture_classname texture_cropped_sample_10 \\\n",
    "--white_classname viton_360t_sample_10_white_mix0_7 \\\n",
    "--target \"red\" \\\n",
    "--workers 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æµ‹è¯•èƒ½å¦ä»ç™½è‰²è¡£æœç”Ÿæˆçº¹ç†ä¸°å¯Œçš„å½©è‰²è¡£æœï¼ˆæ¨¡å‹è®­ç»ƒæ•°æ®çš„mixæ˜¯0-7ï¼‰\n",
    "!CUDA_VISIBLE_DEVICES=0 python scripts/inference_texture_white.py \\\n",
    "--stylegan_weights ../editGAN/viton_360t.pt \\\n",
    "--stylegan_size 256 \\\n",
    "--checkpoint_path checkpoints/images_sample_200000/texture_cropped_sample_200000/net_150000.pth \\\n",
    "--texture_classname mini_textures_for_test \\\n",
    "--white_classname viton_360t_sample_10_white_mix0_7 \\\n",
    "--target \"red\" \\\n",
    "--workers 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stylegan weights from pretrained!\n",
      "completedğŸ‘! Please check results in output/retrain_on_viton\n"
     ]
    }
   ],
   "source": [
    "#æµ‹è¯•èƒ½å¦ç”¨é‡æ–°è®­ç»ƒçš„åŸºäºvitonçš„æ¨¡å‹ä»ç™½è‰²è¡£æœç”Ÿæˆçº¹ç†ä¸°å¯Œçš„å½©è‰²è¡£æœï¼ˆæ¨¡å‹è®­ç»ƒæ•°æ®çš„mixæ˜¯0-7ï¼‰\n",
    "!CUDA_VISIBLE_DEVICES=0 python scripts/inference_texture_white.py \\\n",
    "--stylegan_weights ../editGAN/viton_360t.pt \\\n",
    "--stylegan_size 256 \\\n",
    "--checkpoint_path checkpoints/retrain/images_sample_200000/texture_cropped_sample_200000/net_150000.pth \\\n",
    "--texture_classname mini_textures_for_test \\\n",
    "--white_classname viton_360t_sample_10_white_mix0_7 \\\n",
    "--target \"red\" \\\n",
    "--workers 0 \\\n",
    "--save_dir output/retrain_on_viton"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stylegan weights from pretrained!\n",
      "completedğŸ‘! Please check results in output/retrain_on_viton/chage_w\n"
     ]
    }
   ],
   "source": [
    "#æµ‹è¯•èƒ½å¦ç”¨é‡æ–°è®­ç»ƒçš„åŸºäºvitonçš„æ¨¡å‹ä»ç™½è‰²è¡£æœç”Ÿæˆçº¹ç†ä¸°å¯Œçš„å½©è‰²è¡£æœï¼ˆæ¨¡å‹è®­ç»ƒæ•°æ®çš„mixæ˜¯0-7ï¼‰ ä»¤wç»Ÿä¸€ä¸ºé‡‡æ ·çš„ç¬¬ä¸€ä»¶è¡£æœ\n",
    "!CUDA_VISIBLE_DEVICES=0 python scripts/inference_texture_white.py \\\n",
    "--stylegan_weights ../editGAN/viton_360t.pt \\\n",
    "--stylegan_size 256 \\\n",
    "--checkpoint_path checkpoints/retrain/images_sample_200000/texture_cropped_sample_200000/net_150000.pth \\\n",
    "--texture_classname mini_textures_for_test \\\n",
    "--white_classname viton_360t_sample_10_white_mix0_7 \\\n",
    "--images_classname images_sample_200000 \\\n",
    "--target \"red\" \\\n",
    "--workers 0 \\\n",
    "--save_dir output/retrain_on_viton/chage_w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æµ‹è¯•èƒ½å¦ä»ç™½è‰²è¡£æœç”Ÿæˆçº¹ç†ä¸°å¯Œçš„å½©è‰²è¡£æœï¼ˆæ¨¡å‹è®­ç»ƒæ•°æ®çš„mixæ˜¯0-4,æ¨¡å‹æ˜¯620tï¼‰\n",
    "!CUDA_VISIBLE_DEVICES=0 python scripts/inference_texture_white.py \\\n",
    "--stylegan_weights ../editGAN/cloth-v2-620t.pt \\\n",
    "--stylegan_size 256 \\\n",
    "--checkpoint_path checkpoints/620t_sample_200000/texture_cropped_sample_200000_620t/net_150000.pth \\\n",
    "--texture_classname mini_textures_for_test \\\n",
    "--white_classname 620t_sample_10_white_0_4 \\\n",
    "--target \"red\" \\\n",
    "--workers 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æµ‹è¯•èƒ½å¦ä»ç™½è‰²è¡£æœç”Ÿæˆçš„å½©è‰²è¡£æœï¼ˆæ¨¡å‹è®­ç»ƒæ•°æ®çš„mixæ˜¯0-4,æ¨¡å‹æ˜¯620tï¼‰\n",
    "!CUDA_VISIBLE_DEVICES=0 python scripts/inference_texture_white.py \\\n",
    "--stylegan_weights ../editGAN/cloth-v2-620t.pt \\\n",
    "--stylegan_size 256 \\\n",
    "--checkpoint_path checkpoints/620t_sample_200000/texture_cropped_sample_200000_620t/net_150000.pth \\\n",
    "--texture_classname texture_cropped_sample_10 \\\n",
    "--white_classname 620t_sample_10_white_0_4 \\\n",
    "--target \"red\" \\\n",
    "--workers 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æµ‹è¯•èƒ½å¦ä»ç™½è‰²è¡£æœç”Ÿæˆçº¹ç†ä¸°å¯Œçš„å½©è‰²è¡£æœï¼ˆæ¨¡å‹è®­ç»ƒæ•°æ®çš„mixæ˜¯0-4,æ¨¡å‹æ˜¯620t,fineå’Œmediumçš„lossåˆ†å¼€è®¡ç®—ï¼‰\n",
    "!CUDA_VISIBLE_DEVICES=0 python scripts/inference_texture_white.py \\\n",
    "--stylegan_weights ../editGAN/cloth-v2-620t.pt \\\n",
    "--stylegan_size 256 \\\n",
    "--checkpoint_path checkpoints/modi_loss/620t_sample_200000/texture_cropped_sample_200000_620t/net_150000.pth \\\n",
    "--texture_classname mini_textures_for_test \\\n",
    "--white_classname 620t_sample_10_white_0_4 \\\n",
    "--target \"red\" \\\n",
    "--workers 0 \\\n",
    "--modi_loss True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æµ‹è¯•èƒ½å¦ä»ç™½è‰²è¡£æœç”Ÿæˆçº¹ç†ä¸°å¯Œçš„å½©è‰²è¡£æœï¼ˆæ¨¡å‹è®­ç»ƒæ•°æ®çš„mixæ˜¯0-4,æ¨¡å‹æ˜¯620t,åªå­¦ä¹ mediumï¼‰\n",
    "!CUDA_VISIBLE_DEVICES=0 python scripts/inference_texture_white.py \\\n",
    "--stylegan_weights ../editGAN/cloth-v2-620t.pt \\\n",
    "--stylegan_size 256 \\\n",
    "--checkpoint_path checkpoints/only_medium/620t_sample_200000/texture_cropped_sample_200000_620t/net_150000.pth \\\n",
    "--texture_classname mini_textures_for_test \\\n",
    "--white_classname 620t_sample_10_white_0_4 \\\n",
    "--target \"red\" \\\n",
    "--workers 0 \\\n",
    "--only_medium True"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#æµ‹è¯•èƒ½å¦ä» éšæœºmixå¹³å‡çš„è¡£æœ ç”Ÿæˆçº¹ç†ä¸°å¯Œçš„å½©è‰²è¡£æœï¼ˆæ¨¡å‹è®­ç»ƒæ•°æ®çš„mixæ˜¯0-4,æ¨¡å‹æ˜¯620tï¼‰\n",
    "# white_classnameå–çš„æ˜¯é‡‡æ ·imageçš„classï¼Œåœ¨æ•°æ®é¢„å¤„ç†æ­¥éª¤ä¼šå°†å…¶ä¸avgçš„ç‰¹å¾æ··åˆ\n",
    "!CUDA_VISIBLE_DEVICES=0 python scripts/inference_texture_avg.py \\\n",
    "--stylegan_weights ../editGAN/cloth-v2-620t.pt \\\n",
    "--stylegan_size 256 \\\n",
    "--checkpoint_path checkpoints/avg/620t_sample_200000/texture_cropped_sample_200000_620t/net_150000.pth \\\n",
    "--texture_classname mini_textures_for_test \\\n",
    "--images_classname 620t_sample_200000 \\\n",
    "--target \"red\" \\\n",
    "--workers 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_s\n",
    "#æµ‹è¯•èƒ½å¦ä» éšæœºmixå¹³å‡çš„è¡£æœ ç”Ÿæˆçº¹ç†ä¸°å¯Œçš„å½©è‰²è¡£æœï¼ˆæ¨¡å‹è®­ç»ƒæ•°æ®çš„mixæ˜¯0-4,æ¨¡å‹æ˜¯620tï¼‰\n",
    "# white_classnameå–çš„æ˜¯é‡‡æ ·imageçš„classï¼Œåœ¨æ•°æ®é¢„å¤„ç†æ­¥éª¤ä¼šå°†å…¶ä¸avgçš„ç‰¹å¾æ··åˆ\n",
    "!CUDA_VISIBLE_DEVICES=0 python scripts/inference_texture_avg_all_s.py \\\n",
    "--stylegan_weights ../editGAN/cloth-v2-620t.pt \\\n",
    "--stylegan_size 256 \\\n",
    "--checkpoint_path checkpoints/avg_all_s/620t_sample_200000_all_s/texture_cropped_sample_200000_620t/net_150000.pth \\\n",
    "--texture_classname mini_textures_for_test \\\n",
    "--images_classname 620t_sample_200000_all_s \\\n",
    "--target \"red\" \\\n",
    "--workers 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_s\n",
    "#æµ‹è¯•èƒ½å¦ä» éšæœºmixå¹³å‡çš„è¡£æœ ç”Ÿæˆçº¹ç†ä¸°å¯Œçš„å½©è‰²è¡£æœï¼ˆæ¨¡å‹è®­ç»ƒæ•°æ®çš„mixæ˜¯0-4,æ¨¡å‹æ˜¯620tï¼‰\n",
    "# white_classnameå–çš„æ˜¯å¹³å‡å€¼è¡£æœï¼Œåœ¨æ•°æ®é¢„å¤„ç†æ­¥éª¤ä¼šå°†å…¶ä¸é‡‡æ ·è¡£æœçš„ç‰¹å¾æ··åˆ\n",
    "!CUDA_VISIBLE_DEVICES=0 python scripts/inference_texture_avg_all_s.py \\\n",
    "--stylegan_weights ../editGAN/cloth-v2-620t.pt \\\n",
    "--stylegan_size 256 \\\n",
    "--checkpoint_path checkpoints/avg_all_s/620t_sample_200000_all_s/texture_cropped_sample_200000_620t/net_150000.pth \\\n",
    "--texture_classname mini_textures_for_test \\\n",
    "--images_classname 620t_sample_200000_all_s \\\n",
    "--white_classname 620t_avg_latent_all_s \\\n",
    "--target \"red\" \\\n",
    "--workers 0 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ç”±fashionTexçš„çº¹ç†å›¾ç”Ÿæˆclipç‰¹å¾ï¼ˆ620tï¼‰\n",
    "!CUDA_VISIBLE_DEVICES=0 python get_clip_latent.py --classname fashion_tex_texture \\\n",
    "    --image_dir ../editGANdata/fashiontex_texture \\\n",
    "    --nums 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#all_s, extra mapperï¼Œimg loss\n",
    "#æµ‹è¯•èƒ½å¦ä» éšæœºmixå¹³å‡çš„è¡£æœ æ ¹æ®fashionTexçš„çº¹ç†ç”Ÿæˆå½©è‰²è¡£æœï¼ˆæ¨¡å‹è®­ç»ƒæ•°æ®çš„mixæ˜¯0-4,æ¨¡å‹æ˜¯620tï¼‰\n",
    "# white_classnameå–çš„æ˜¯å¹³å‡å€¼è¡£æœï¼Œåœ¨æ•°æ®é¢„å¤„ç†æ­¥éª¤ä¼šå°†å…¶ä¸é‡‡æ ·è¡£æœçš„ç‰¹å¾æ··åˆ\n",
    "!CUDA_VISIBLE_DEVICES=0 python scripts/inference_texture_avg_all_s_extra_mapper_img_loss.py \\\n",
    "--stylegan_weights ../editGAN/cloth-v2-620t.pt \\\n",
    "--stylegan_size 256 \\\n",
    "--checkpoint_path checkpoints/avg_all_s_extra_mapper_img_loss/620t_sample_200000_all_s/texture_cropped_sample_200000_620t/net_1920000.pth \\\n",
    "--texture_classname fashion_tex_texture \\\n",
    "--images_classname 620t_sample_200000_all_s \\\n",
    "--white_classname 620t_avg_latent_all_s \\\n",
    "--target \"red\" \\\n",
    "--workers 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading stylegan weights from pretrained!\n",
      "Traceback (most recent call last):\n",
      "  File \"scripts/inference_fashion_tex_texture_mapper.py\", line 71, in <module>\n",
      "    main(opts)\n",
      "  File \"scripts/inference_fashion_tex_texture_mapper.py\", line 44, in main\n",
      "    g_ema = g_ema.to(device)\n",
      "  File \"/home/scut/workspace/anaconda3/envs/deltaEdit/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 612, in to\n",
      "    return self._apply(convert)\n",
      "  File \"/home/scut/workspace/anaconda3/envs/deltaEdit/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 359, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/scut/workspace/anaconda3/envs/deltaEdit/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 359, in _apply\n",
      "    module._apply(fn)\n",
      "  File \"/home/scut/workspace/anaconda3/envs/deltaEdit/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 381, in _apply\n",
      "    param_applied = fn(param)\n",
      "  File \"/home/scut/workspace/anaconda3/envs/deltaEdit/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 610, in convert\n",
      "    return t.to(device, dtype if t.is_floating_point() else None, non_blocking)\n",
      "RuntimeError: CUDA error: out of memory\n"
     ]
    }
   ],
   "source": [
    "#æµ‹è¯•å°†mapperæ”¹ä¸ºfashionTexçš„mepperçš„æ•ˆæœ\n",
    "#æµ‹è¯•èƒ½å¦ä» éšæœºè¡£æœ ç”Ÿæˆçº¹ç†ä¸°å¯Œçš„å½©è‰²è¡£æœï¼ˆæ¨¡å‹æ˜¯620tï¼‰\n",
    "!CUDA_VISIBLE_DEVICES=0 python scripts/inference_fashion_tex_texture_mapper.py \\\n",
    "--stylegan_weights ../editGAN/cloth-v2-620t.pt \\\n",
    "--stylegan_size 256 \\\n",
    "--checkpoint_path checkpoints/fashion_tex_texture_mapper/620t_sample_200000_all_s/texture_cropped_sample_200000_620t/net_400000.pth \\\n",
    "--texture_classname mini_textures_for_test \\\n",
    "--images_classname 620t_sample_200000_all_s \\\n",
    "--white_classname xx \\\n",
    "--target \"red\" \\\n",
    "--workers 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
