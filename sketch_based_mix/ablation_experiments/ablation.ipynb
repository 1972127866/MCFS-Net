{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "00000000000cuda_home: /usr/local/cuda-11.1\n",
      "Opt {'exp_dir': '../../hdd/wanqing/editGAN/model_encoder/cloth_batch_8_loss_sampling_train_stylegan2ada', 'category': 'car', 'im_size': [256, 256], 'n_latent': 14, 'lr': 3e-05, 'debug': False, 'bs': 8, 'sample_bs': 2, 'avg_latent': '', 'stylegan_checkpoint': 'cloth-v2-620t-s-test.pkl', 'max_label': 7, 'classfier_checkpoint': '', 'training_data_path': '../data/cloth', 'label_data_path': '', 'testing_data_path': '', 'sampling_training': True, 'use_w': False, 'same_view_code': False, 'truncation': False, 'normalize': True, 'stylegan_ver': '3', 'train_real_start_epochs': 5, 'dim': 4992, 'loss_dict': {'p_loss': 10, 'mse_loss': 1, 'encoder_loss': 5, 'e_label_ce_loss': 1, 'e_label_dice_loss': 1}, 'use_noise': False, 'noise_loss_weight': 100}\n",
      "Loading networks from \"cloth-v2-620t-s-test.pkl\"...\n",
      "Setting up PyTorch plugin \"bias_act_plugin\"... Done.\n",
      " make_mean_latent \n",
      "Setting up Perceptual loss...\n",
      "Loading model from: /home/scut/workspace/wanqing/editGAN/lpips/weights/v0.1/vgg.pth\n",
      "...[net-lin [vgg]] initialized\n",
      "...Done\n",
      "All files,  100\n",
      "  0%|                                                   | 0/100 [00:00<?, ?it/s]Curr dir, test_085\n",
      "****** Run optimization for  ../../hdd/wanqing/editGAN/model_encoder/cloth_batch_8_loss_sampling_train_stylegan2ada  ******\n",
      "SV folder at: /home/scut/workspace/wanqing/editGANresult/embed_wo_noise_term/fashionTOP_testset100/w-step400_s-step250\n",
      "****** Run optimization for  /home/scut/workspace/wanqing/editGANdata/fashionTOP_testset100/test_085.png  ******\n",
      "latent_in.shape torch.Size([1, 14, 512])\n",
      "Setting up PyTorch plugin \"upfirdn2d_plugin\"... Done.\n",
      "100%|█████████████████████████████████████████| 399/399 [01:09<00:00,  5.76it/s]\n",
      "Curr loss,  2.9242820739746094 0.7351210117340088\n",
      "Computing W midpoint and stddev using 100 samples...\n",
      "  0%|                                                   | 0/100 [01:10<?, ?it/s]\n",
      "Traceback (most recent call last):\n",
      "  File \"embed-project_wo_noise_term.py\", line 437, in <module>\n",
      "    test_stylegan_proj(opts, args.resume, args.steps,num_steps=args.num_steps,\n",
      "  File \"embed-project_wo_noise_term.py\", line 332, in test_stylegan_proj\n",
      "    projected_w_steps = project(\n",
      "  File \"embed-project_wo_noise_term.py\", line 131, in project\n",
      "    img, styles_feature, s_samples = G.module.synthesis(w_samples, noise_mode='const', return_style=True)\n",
      "  File \"/home/scut/anaconda3/envs/editGAN/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"<string>\", line 527, in forward\n",
      "  File \"/home/scut/anaconda3/envs/editGAN/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"<string>\", line 443, in forward\n",
      "  File \"/home/scut/anaconda3/envs/editGAN/lib/python3.8/site-packages/torch/nn/modules/module.py\", line 889, in _call_impl\n",
      "    result = self.forward(*input, **kwargs)\n",
      "  File \"<string>\", line 303, in forward\n",
      "  File \"/home/scut/workspace/wanqing/editGAN/torch_utils/misc.py\", line 101, in decorator\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"<string>\", line 66, in modulated_conv2d\n",
      "  File \"/home/scut/workspace/wanqing/editGAN/torch_utils/misc.py\", line 101, in decorator\n",
      "    return fn(*args, **kwargs)\n",
      "  File \"/home/scut/workspace/wanqing/editGAN/torch_utils/ops/conv2d_resample.py\", line 147, in conv2d_resample\n",
      "    return _conv2d_wrapper(x=x, w=w, padding=[py0,px0], groups=groups, flip_weight=flip_weight)\n",
      "  File \"/home/scut/workspace/wanqing/editGAN/torch_utils/ops/conv2d_resample.py\", line 54, in _conv2d_wrapper\n",
      "    return op(x, w, stride=stride, padding=padding, groups=groups)\n",
      "  File \"/home/scut/workspace/wanqing/editGAN/torch_utils/ops/conv2d_gradfix.py\", line 38, in conv2d\n",
      "    return torch.nn.functional.conv2d(input=input, weight=weight, bias=bias, stride=stride, padding=padding, dilation=dilation, groups=groups)\n",
      "RuntimeError: CUDA out of memory. Tried to allocate 800.00 MiB (GPU 0; 23.70 GiB total capacity; 4.46 GiB already allocated; 633.69 MiB free; 5.97 GiB reserved in total by PyTorch)\n"
     ]
    }
   ],
   "source": [
    "!python embed-project_wo_noise_term.py --steps=400 --num_steps=250 --exp experiments/encoder_cloth.json \\\n",
    "    --resume /home/scut/hdd/wanqing/editGAN/model_encoder/cloth_batch_8_loss_sampling_train_stylegan2ada/checkpoint/BEST_loss2.5923986434936523.pth \\\n",
    "    --testing_path /home/scut/workspace/wanqing/editGANdata/fashionTOP_testset100 \\\n",
    "    --latent_sv_folder /home/scut/workspace/wanqing/editGANresult/embed_wo_noise_term/fashionTOP_testset100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
